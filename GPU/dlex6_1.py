# -*- coding: utf-8 -*-
"""DLex6-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sy25ut29B3YnYknYKpaOOpwJLfTNlfG5
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN
from tensorflow.keras.callbacks import EarlyStopping
imdb

(x_trainn, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)
maxlen = 500
x_train = tf.keras.preprocessing.sequence.pad_sequences(x_trainn, maxlen=maxlen)
x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)
word_index = imdb.get_word_index()

embeddings_index = dict()
f = open('/content/glove.6B.100d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:])
    embeddings_index[word] = coefs
f.close()

size = 10000 
embedding_matrix = np.zeros((size, 100))
for word, index in word_index.items():
    if index > size - 1:
        break
    else:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector

model = Sequential()
model.add(Embedding(size, 100, input_length=500, weights=[embedding_matrix], trainable=False))
model.add(SimpleRNN(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)
model.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test, y_test), callbacks=[stop])

model.fit(x_train, y_train, epochs=5, batch_size=128, validation_data=(x_test, y_test))

import tensorflow as tf
from datetime import datetime
logdir = "logs/text_basics/" + datetime.now().strftime("%Y%m%d-%H%M%S")
file_writer = tf.summary.create_file_writer(logdir)
with file_writer.as_default():
  for word, index in word_index.items():
    tf.summary.text("first_text", word, step=0)

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir logs